{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Union\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pyrallis\n",
    "from  util.model import DINOHead\n",
    "\n",
    "# from safetensors import safe_open\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    # StableDiffusionPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "import torch\n",
    "from config import RunConfig\n",
    "from util import ptp_utils\n",
    "from util.pipeline_attend_and_excite import AttendAndExcitePipeline\n",
    "from util.ptp_utils import AttentionStore\n",
    "from util.pipeline_attend_and_excite import TextaulStableDiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer,CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e00f1e1bdf4401a923ab0393eebef34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TextaulStableDiffusionPipeline {\n",
       "  \"_class_name\": \"TextaulStableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.20.2\",\n",
       "  \"_name_or_path\": \"stabilityai/stable-diffusion-2-1\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"requires_safety_checker\": false,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"DDIMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/opt/data/private/stable_diffusion_model\"\n",
    "textual_name = \"textual_inversion_find_new_style_13\"\n",
    "placeholder_token = '<style>'\n",
    "nums_token = 768\n",
    "#\"runwayml/stable-diffusion-v1-5\"\n",
    "#stabilityai/stable-diffusion-2-1-base\n",
    "#stabilityai/stable-diffusion-2-1\n",
    "#stabilityai/stable-diffusion-xl-base-1.0\n",
    "model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "moldel_root = f'{root}/{textual_name}/model_5000.pt'\n",
    "state_dict = torch.load(moldel_root)['model']\n",
    "backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16',map_location='cpu')\n",
    "head = DINOHead(in_dim=768, out_dim=1024,cls_dim=6, nlayers=3)\n",
    "image_model = torch.nn.Sequential(backbone, head)\n",
    "image_model.load_state_dict(state_dict)\n",
    "name =''\n",
    "for x in model_id.split('/'):\n",
    "    name += x+'_'\n",
    "name =name+f\"pca_{nums_token}.pt\"\n",
    "save_path = os.path.join(\"token_dict\", name)\n",
    "token_dict = torch.load(save_path)\n",
    "pipe = TextaulStableDiffusionPipeline.from_pretrained(model_id,torch_dtype=torch.float16)\n",
    "pipe.unet.enable_xformers_memory_efficient_attention()\n",
    "setattr(pipe, 'image_model', None)\n",
    "setattr(pipe, 'token_dict', None)\n",
    "pipe.image_model = image_model\n",
    "pipe.token_dict = token_dict\n",
    "pipe.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0330, -0.1685,  0.0798,  ...,  0.0277,  0.3194, -0.0250]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 14.2816,  12.3414,  -8.9154,   2.7792,   7.8765, -13.5120]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "image_size = 224\n",
    "crop_pct = 0.875\n",
    "interpolation = 3\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize(int(image_size / crop_pct),interpolation=interpolation),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225]),])\n",
    "image_path = 'datasets/sub_style_dataset/Baroque/adriaen-van-de-velde_agony-in-the-garden-1665.jpg'\n",
    "referenceImage = Image.open(image_path)\n",
    "referenceImage = referenceImage.convert(\"RGB\")\n",
    "referenceImage = trans(referenceImage)\n",
    "referenceImage = referenceImage.unsqueeze(0)\n",
    "referenceImage = referenceImage.cuda()\n",
    "image_model.cuda() \n",
    "with torch.no_grad():\n",
    "    weight,logits = image_model(referenceImage)\n",
    "print(weight)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_prompt(prompt:Union[str, List[str]],\n",
    "                  model: TextaulStableDiffusionPipeline,\n",
    "                  controller: AttentionStore,\n",
    "                  token_indices: List[int],\n",
    "                  seed: torch.Generator,\n",
    "                  config: RunConfig,\n",
    "                  referenceImages: List[torch.Tensor] = None,\n",
    "                  referenceLocattion: List[int] = None,) -> Image.Image:\n",
    "    if controller is not None:\n",
    "        ptp_utils.register_attention_control(model, controller)\n",
    "    if isinstance(model,AttendAndExcitePipeline):\n",
    "        outputs = model(prompt=prompt,\n",
    "                        attention_store=controller,\n",
    "                        indices_to_alter=token_indices,\n",
    "                        attention_res=config.attention_res,\n",
    "                        guidance_scale=config.guidance_scale,\n",
    "                        generator=seed,\n",
    "                        num_inference_steps=config.n_inference_steps,\n",
    "                        max_iter_to_alter=config.max_iter_to_alter,\n",
    "                        run_standard_sd=config.run_standard_sd,\n",
    "                        thresholds=config.thresholds,\n",
    "                        scale_factor=config.scale_factor,\n",
    "                        scale_range=config.scale_range,\n",
    "                        smooth_attentions=config.smooth_attentions,\n",
    "                        sigma=config.sigma,\n",
    "                        kernel_size=config.kernel_size,\n",
    "                        sd_2_1=config.sd_2_1)\n",
    "    else:\n",
    "        outputs = model(prompt=prompt,\n",
    "                        referenceImages = referenceImages,\n",
    "                        referenceLocattion = referenceLocattion,\n",
    "                        num_inference_steps = config.n_inference_steps,\n",
    "                        guidance_scale = config.guidance_scale,\n",
    "                        generator=seed)\n",
    "    image = outputs.images[0]\n",
    "    return image\n",
    "\n",
    "@pyrallis.wrap()\n",
    "def main(config: RunConfig):\n",
    "    prompts = [\n",
    "            \"a photo of a dog in the style of S\",\n",
    "            # \"a photo of the <style>_1,4k\",\n",
    "            # \"a photo of the <style>_2,4k\",\n",
    "            # \"a photo of the <style>_3,4k\",\n",
    "            # \"a photo of the <style>_4,4k\",\n",
    "            # \"a photo of the <style>_5,4k\",\n",
    "            # \"a photo of the <style>_6,4k\",\n",
    "            # \"a photo of the <style>_7,4k\",\n",
    "            # \"a photo of the <style>_8,4k\",\n",
    "            # \"a photo of the <style>_9,4k\",\n",
    "            # \"a photo of the <object>_9 is on the grass\",\n",
    "            ] \n",
    "    # objiect_name = [\"panda\",\"cat\",\"dog\",\"anemone fish\",\"hen\",\"bee eater\",\"box turtle\",\"African elephant\",\"rat\",\"lion\"]\n",
    "    # for name in objiect_name:\n",
    "    #     prompts.append(f\"A {name} is drinking water,4k\")   \n",
    "    image_size = 224\n",
    "    crop_pct = 0.875\n",
    "    interpolation = 3\n",
    "    trans = transforms.Compose([\n",
    "        transforms.Resize(int(image_size / crop_pct),interpolation=interpolation),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225]),])\n",
    "    image_path = 'datasets/sub_style_dataset/Baroque/adriaen-van-de-velde_agony-in-the-garden-1665.jpg'\n",
    "    referenceImage = Image.open(image_path)\n",
    "    referenceImage = referenceImage.convert(\"RGB\")\n",
    "    image_name = image_path.split('/')[-1].split('.')[0]\n",
    "    referenceImage = trans(referenceImage)\n",
    "    # image_model.cuda()\n",
    "    # referenceImage = referenceImage.unsqueeze(0).cuda()\n",
    "    # weight,logit = image_model(referenceImage)\n",
    "    referenceImage = [referenceImage]\n",
    "    token_indices = [9]\n",
    "    # referenceImage =[]\n",
    "    # token_indices = []\n",
    "    controller = AttentionStore()\n",
    "    for prompt in prompts:\n",
    "        os.makedirs(f\"./images/{textual_name}/{prompt}/{image_name}\",exist_ok=True)\n",
    "        for seed in range(0,5):\n",
    "            g = torch.Generator('cuda').manual_seed(seed)\n",
    "            image = run_on_prompt(prompt=prompt,\n",
    "                                model=pipe,\n",
    "                                controller=controller,\n",
    "                                token_indices=token_indices,\n",
    "                                seed=g,\n",
    "                                config=config,\n",
    "                                referenceImages = referenceImage,\n",
    "                                referenceLocattion = token_indices)\n",
    "            image.save(f\"./images/{textual_name}/{prompt}/{image_name}/{seed}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config_path str] [--prompt str]\n",
      "                             [--sd_2_1 str] [--token_indices str]\n",
      "                             [--seeds str] [--n_inference_steps str]\n",
      "                             [--guidance_scale str] [--max_iter_to_alter str]\n",
      "                             [--attention_res str] [--run_standard_sd str]\n",
      "                             [--thresholds str] [--scale_factor str]\n",
      "                             [--scale_range str] [--smooth_attentions str]\n",
      "                             [--sigma str] [--kernel_size str]\n",
      "                             [--save_cross_attention_maps str]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/root/.local/share/jupyter/runtime/kernel-v2-6024DpgIWwROocUR.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/xformers/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
